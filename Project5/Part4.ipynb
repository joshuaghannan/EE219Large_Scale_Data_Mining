{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pytz\n",
    "%matplotlib inline\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training and Testing directories\n",
    "training_dir = os.path.join(\"Datasets\", \"Training\")\n",
    "testing_dir = os.path.join(\"Datasets\", \"Testing\")\n",
    "if not os.path.isdir(training_dir):\n",
    "    raise Exception(\"ERROR: training dataset not found\")\n",
    "if not os.path.isdir(testing_dir):\n",
    "    raise Exception(\"ERROR: testing dataset not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets/Training/tweets_#gohawks.txt\n",
      "Datasets/Training/tweets_#gopatriots.txt\n",
      "Datasets/Training/tweets_#nfl.txt\n",
      "Datasets/Training/tweets_#patriots.txt\n",
      "Datasets/Training/tweets_#sb49.txt\n",
      "Datasets/Training/tweets_#superbowl.txt\n"
     ]
    }
   ],
   "source": [
    "# iterate over all hashtag files \n",
    "for root, dirs, files in os.walk(training_dir, topdown=False):\n",
    "    for file in files:\n",
    "        print(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize dictionaries and Unix times for Feb 1, 8 am and Feb 1, 8 pm. \n",
    "# Dictionary keys: hashtag.\n",
    "# Dictionary values: [time of tweet (Unix), number of retweets for tweet, number of followers for tweeter]\n",
    "# Each row in dictionary value is an individual tweet.\n",
    "\n",
    "hashtag_dict_before = {}\n",
    "hashtag_dict_during = {}\n",
    "hashtag_dict_after = {}\n",
    "start_unix_time = 1422806400 # 8 am, Feb 1, PST\n",
    "end_unix_time = 1422849600 # 8 pm, Feb 1, PST\n",
    "pst_tz = pytz.timezone('America/Los_Angeles')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing gohawks...\n",
      "Parsing gopatriots...\n",
      "Parsing nfl...\n",
      "Parsing patriots...\n",
      "Parsing sb49...\n",
      "Parsing superbowl...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Parse files to get necessary data \"\"\"\n",
    "\n",
    "for root, dirs, files in os.walk(training_dir, topdown=False):\n",
    "    for file in files:\n",
    "        filename = os.path.splitext(file)[0].replace('tweets_#', '')\n",
    "        print('Parsing {}...'.format(filename))\n",
    "        \n",
    "        hashtag_dict_before[filename] = []\n",
    "        hashtag_dict_during[filename] = []\n",
    "        hashtag_dict_after[filename] = []\n",
    "        \n",
    "        # open the file and read all lines:\n",
    "        with open(os.path.join(root, file), \"r\", encoding=\"utf-8\") as hashtag:\n",
    "            # read line-by-line\n",
    "            for line in hashtag:\n",
    "                json_obj = json.loads(line)\n",
    "                \n",
    "                # get desired statistics\n",
    "                citation_date = json_obj['citation_date'] # Unix time\n",
    "                num_retweets = json_obj['metrics']['citations']['total'] # Number of retweets for this tweet\n",
    "                num_followers = json_obj['author']['followers'] # Number of followers for tweeter\n",
    "                \n",
    "                # Check when tweet was made and add it to corresponding dictionary\n",
    "                if citation_date < start_unix_time:\n",
    "                    hashtag_dict_before[filename].append([citation_date, num_retweets, num_followers])\n",
    "                elif citation_date > end_unix_time:\n",
    "                    hashtag_dict_after[filename].append([citation_date, num_retweets, num_followers])\n",
    "                else:\n",
    "                    hashtag_dict_during[filename].append([citation_date, num_retweets, num_followers])\n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organize Data\n",
    "\n",
    "##### Variables:\n",
    "<span>\n",
    "key = one of the hashtags <br \\> <br \\>\n",
    "</span>\n",
    "\n",
    "<span>\n",
    "data_hashtag_before[key] = data before 2/1 8am, split into 1-hour windows (separated by hashtag) <br \\>\n",
    "data_hashtag_during[key] = data between 2/1 8am and 8pm, split into 5-min windows (separated by hashtag) <br \\>\n",
    "data_hashtag_after[key] = data after 2/1 8pm, split into 1-hour windows (separated by hashtag) <br \\> <br \\>\n",
    "</span>\n",
    "\n",
    "<span>\n",
    "data_aggregate_before = data before 2/1 8am, split into 1-hour windows (all hashtags combined) <br \\>\n",
    "data_aggregate_during = data between 2/1 8am and 8pm, split into 5-min windows (all hashtags combined) <br \\>\n",
    "data_aggregate_after = data after 2/1 8pm, split into 1-hour windows (all hashtags combined) <br \\> <br \\>\n",
    "</span>\n",
    "\n",
    "<span>\n",
    "data_hashtag_all[key] = all data, split into 1-hour windows (separated by hashtag) <br \\> <br \\>\n",
    "</span>\n",
    "\n",
    "<span>\n",
    "data_all = all data, split into 1-hour windows (all hashtags combined) <br \\>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Explicitly list hashtags. \n",
    "# Convert each value in dictionary to numpy arrays.\n",
    "\n",
    "hashtags = ['gohawks', 'gopatriots', 'nfl', 'patriots', 'sb49', 'superbowl']\n",
    "\n",
    "for key in hashtags:\n",
    "    hashtag_dict_before[key] = np.array(hashtag_dict_before[key])\n",
    "    hashtag_dict_during[key] = np.array(hashtag_dict_during[key])\n",
    "    hashtag_dict_after[key] = np.array(hashtag_dict_after[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find how many time windows there are\n",
    "\n",
    "ftt = int(np.min([np.min(hashtag_dict_before[key][:,0]) for key in hashtags])) # first tweet time\n",
    "ltt = int(np.max([np.max(hashtag_dict_after[key][:,0]) for key in hashtags])) # last tweet time\n",
    "\n",
    "num_windows_before = int(np.max([((start_unix_time - ftt) // 3600) + 1 for key in hashtags]))\n",
    "num_windows_during = int(np.max([((end_unix_time - start_unix_time) // 3600 * 12) for key in hashtags]))\n",
    "num_windows_after = int(np.max([((ltt - end_unix_time) // 3600) + 1 for key in hashtags]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gohawks\n",
      "gopatriots\n",
      "nfl\n",
      "patriots\n",
      "sb49\n",
      "superbowl\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Organize data into specific time periods:\n",
    "     before 2/1 8am with 1-hour windows, \n",
    "     between 2/1 8am and 2/1 8pm with 5-min windows,\n",
    "     and after 2/1 8pm with 1-hour windows \"\"\"\n",
    "\n",
    "# Initialize dictionary for each time frame.\n",
    "data_hashtag_before = {}\n",
    "data_hashtag_during = {}\n",
    "data_hashtag_after = {}\n",
    "\n",
    "# Iterate through each hashtag.\n",
    "for key in hashtags:\n",
    "    print(key)\n",
    "    \n",
    "    # Rename the dictionary value for readability\n",
    "    temp_before = hashtag_dict_before[key]\n",
    "    temp_during = hashtag_dict_during[key]\n",
    "    temp_after = hashtag_dict_after[key]\n",
    "    \n",
    "    data_hashtag_before[key] = np.zeros((num_windows_before, 5)) # Initialize array: rows = time window, columns = feature\n",
    "    num_followers_before = {} # Initialize dictionary to count # of followers for each tweet\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Iterate through all elements before start time\n",
    "    for i in range(np.shape(temp_before)[0]):\n",
    "        # Get row number\n",
    "        item_before = int(num_windows_before - 1 - ((start_unix_time - temp_before[i,0] - 1) // 3600))\n",
    "        # Update first 3 elements (# of tweets, total # retweets, total # followers)\n",
    "        data_hashtag_before[key][item_before] += np.array([1, int(temp_before[i, 1]), int(temp_before[i, 2]), 0, 0])\n",
    "        # Get time of day (hour)\n",
    "        dt_obj_pst = datetime.fromtimestamp(temp_before[i,0], pst_tz)\n",
    "        data_hashtag_before[key][item_before][4] = int(datetime.strftime(dt_obj_pst, '%H'))\n",
    "        # Get number of followers\n",
    "        if item_before not in num_followers_before.keys():\n",
    "            num_followers_before[item_before] = []\n",
    "        num_followers_before[item_before].append(temp_before[i,2])\n",
    "    for i in num_followers_before.keys():\n",
    "        data_hashtag_before[key][i][3] = np.max(num_followers_before[i])\n",
    "        \n",
    "        \n",
    "    # Iterate through all elements during time\n",
    "    data_hashtag_during[key] = np.zeros((num_windows_during, 5))\n",
    "    num_followers_during = {}\n",
    "    for i in range(np.shape(temp_during)[0]):\n",
    "        item_during = int(((temp_during[i,0] - start_unix_time) * 12) // 3600)\n",
    "        data_hashtag_during[key][item_during] += np.array([1, int(temp_during[i, 1]), int(temp_during[i, 2]), 0, 0])\n",
    "        dt_obj_pst = datetime.fromtimestamp(temp_during[i,0], pst_tz)\n",
    "        data_hashtag_during[key][item_during][4] = int(datetime.strftime(dt_obj_pst, '%H'))\n",
    "        \n",
    "        if item_during not in num_followers_during.keys():\n",
    "            num_followers_during[item_during] = []\n",
    "        num_followers_during[item_during].append(temp_during[i,2])\n",
    "    for i in num_followers_during.keys():\n",
    "        data_hashtag_during[key][i][3] = np.max(num_followers_during[i])\n",
    "        \n",
    "    # Iterate through all elements after end time\n",
    "    data_hashtag_after[key] = np.zeros((num_windows_after, 5))\n",
    "    num_followers_after = {}\n",
    "    for i in range(np.shape(temp_after)[0]):\n",
    "        item_after = int((temp_after[i,0] - end_unix_time) // 3600)\n",
    "        data_hashtag_after[key][item_after] += np.array([1, int(temp_after[i, 1]), int(temp_after[i, 2]), 0, 0])\n",
    "        dt_obj_pst = datetime.fromtimestamp(temp_after[i,0], pst_tz)\n",
    "        data_hashtag_after[key][item_after][4] = int(datetime.strftime(dt_obj_pst, '%H'))\n",
    "        \n",
    "        if item_after not in num_followers_after.keys():\n",
    "            num_followers_after[item_after] = []\n",
    "        num_followers_after[item_after].append(temp_after[i,2])\n",
    "    for i in num_followers_after.keys():\n",
    "        data_hashtag_after[key][i][3] = np.max(num_followers_after[i])\n",
    "        \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Aggregate data within each time period by combining all hashtags. \"\"\"\n",
    "\n",
    "# Initialize aggregated data variables\n",
    "data_aggregate_before = np.zeros([num_windows_before, 5])\n",
    "data_aggregate_during = np.zeros([num_windows_during, 5])\n",
    "data_aggregate_after = np.zeros([num_windows_after, 5])\n",
    "\n",
    "# Sum the # of tweets, total # of retweets, and # of followers\n",
    "for key in hashtags:\n",
    "    data_aggregate_before[:,0:3] += data_hashtag_before[key][:,0:3]\n",
    "    data_aggregate_during[:,0:3] += data_hashtag_during[key][:,0:3]\n",
    "    data_aggregate_after[:,0:3] += data_hashtag_after[key][:,0:3]\n",
    "# Find the max # of followers for each\n",
    "data_aggregate_before[:,3] = np.amax([data_hashtag_before[key][:,3] for key in hashtags], axis=0)\n",
    "data_aggregate_during[:,3] = np.amax([data_hashtag_during[key][:,3] for key in hashtags], axis=0)\n",
    "data_aggregate_after[:,3] = np.amax([data_hashtag_after[key][:,3] for key in hashtags], axis=0)\n",
    "\n",
    "# Copy over the same time frames\n",
    "data_aggregate_before[:,4] = data_hashtag_before['superbowl'][:,4]\n",
    "data_aggregate_during[:,4] = data_hashtag_during['superbowl'][:,4]\n",
    "data_aggregate_after[:,4] = data_hashtag_after['superbowl'][:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Get data for the whole time frame with 1-hour windows, separated by hashtag \"\"\"\n",
    "\n",
    "# Initialize dictionary to store data.\n",
    "# Key: hashtag\n",
    "# Value: data separated by 1-hour time windows\n",
    "data_hashtag_all = {}\n",
    "\n",
    "for key in hashtags: # Iterate through all hashtags\n",
    "    temp_during = np.zeros([12, 5]) # Initialize array to store data in the middle time period\n",
    "    # Combine data in the middle time period\n",
    "    for i in range(np.shape(data_hashtag_during[key])[0]):\n",
    "        hour = int(data_hashtag_during[key][i,4] - 8)\n",
    "        temp_during[hour, :3] += data_hashtag_during[key][i, :3]\n",
    "        if not i % 12:\n",
    "            temp_during[hour, 3] = np.max(data_hashtag_during[key][i:(i+12), 3])\n",
    "            temp_during[hour, 4] = data_hashtag_during[key][i,4]\n",
    "    data_hashtag_all[key] = np.vstack((data_hashtag_before[key], temp_during, data_hashtag_after[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Combine data for the whole time frame from all hashtags \"\"\"\n",
    "\n",
    "data_all = np.zeros([587, 5])\n",
    "for key in hashtags:\n",
    "    data_all += data_hashtag_all[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define layer sizes.\n",
    "layer_sizes = [(50, 50), (100, 100), (100, 100, 100), (100, 100, 100, 100), 10*(50,), 10*(100,)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to get MSE for a given neural network model.\n",
    "\n",
    "def analyze_nn(nn, X, y):\n",
    "    mses_per_fold = []\n",
    "    mses_per_fold_train = []\n",
    "    kf = KFold(10)\n",
    "    for trainset, testset in kf.split(X):\n",
    "        X_train, y_train = X[trainset], y[trainset]\n",
    "        X_test, y_test = X[testset], y[testset]\n",
    "        nn.fit(X_train, y_train)\n",
    "        predicted = nn.predict(X_test)\n",
    "        mses_per_fold.append(mean_squared_error(y_test, predicted))\n",
    "        mses_per_fold_train.append(mean_squared_error(y_train, nn.predict(X_train)))\n",
    "    #     print(mean_squared_error(y_test, predicted))\n",
    "    avg_mse = np.mean(mses_per_fold)\n",
    "    avg_mse_train = np.mean(mses_per_fold_train)\n",
    "    print('Layer size {} MSE:\\n   val = {} \\n train = {}'.format(size, np.around(avg_mse, 2), np.around(avg_mse_train, 2)))\n",
    "    \n",
    "    return avg_mse, avg_mse_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (586, 5)\n",
      "y shape: (586,)\n",
      "Layer size (50, 50) MSE:\n",
      "   val = 59745874037.11 \n",
      " train = 9108725423.43\n",
      "Layer size (100, 100) MSE:\n",
      "   val = 3508811621.05 \n",
      " train = 281766647889.0\n",
      "Layer size (100, 100, 100) MSE:\n",
      "   val = 10658749042.71 \n",
      " train = 30772545538.35\n",
      "Layer size (100, 100, 100, 100) MSE:\n",
      "   val = 8591186972.54 \n",
      " train = 8205157949.7\n",
      "Layer size (50, 50, 50, 50, 50, 50, 50, 50, 50, 50) MSE:\n",
      "   val = 692571491.51 \n",
      " train = 8027511120.38\n",
      "Layer size (100, 100, 100, 100, 100, 100, 100, 100, 100, 100) MSE:\n",
      "   val = 1038210307.55 \n",
      " train = 2809171097.29\n"
     ]
    }
   ],
   "source": [
    "# Analyze tweets before start time.\n",
    "\n",
    "# Define train data and targets\n",
    "y = data_all[1:,0] # Number of tweets (except first)\n",
    "X = np.delete(data_all, -1, 0) # Delete last row\n",
    "\n",
    "# X = X_before\n",
    "# y = y_before\n",
    "\n",
    "print('X shape:', X.shape)\n",
    "print('y shape:', y.shape)\n",
    "\n",
    "mses = []\n",
    "mses_train = []\n",
    "for size in layer_sizes:\n",
    "    nn = MLPRegressor(hidden_layer_sizes=size, activation='relu', solver='adam', alpha=0.001)\n",
    "\n",
    "    avg_mse, avg_mse_train = analyze_nn(nn, X, y)\n",
    "    mses.append(avg_mse)\n",
    "    mses_train.append(avg_mse_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[244429.69139838  59235.22280747 103241.21775098  92688.65611571\n",
      "  26316.75305792  32221.27104179]\n"
     ]
    }
   ],
   "source": [
    "print(np.sqrt(mses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard scaler preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (586, 5)\n",
      "y shape: (586,)\n",
      "Layer size (50, 50) MSE:\n",
      "   val = 780638931.66 \n",
      " train = 644161652.92\n",
      "Layer size (100, 100) MSE:\n",
      "   val = 662703464.28 \n",
      " train = 376914873.93\n",
      "Layer size (100, 100, 100) MSE:\n",
      "   val = 261528210.03 \n",
      " train = 231483313.9\n",
      "Layer size (100, 100, 100, 100) MSE:\n",
      "   val = 319585942.97 \n",
      " train = 189917573.43\n",
      "Layer size (50, 50, 50, 50, 50, 50, 50, 50, 50, 50) MSE:\n",
      "   val = 677650603.25 \n",
      " train = 109547125.55\n",
      "Layer size (100, 100, 100, 100, 100, 100, 100, 100, 100, 100) MSE:\n",
      "   val = 697102371.67 \n",
      " train = 70324031.67\n"
     ]
    }
   ],
   "source": [
    "y = data_all[1:,0] # Number of tweets (except first)\n",
    "X = np.delete(data_all, -1, 0) # Delete last row\n",
    "\n",
    "X = scaler.fit_transform(X) # Transform data\n",
    "y = y\n",
    "\n",
    "print('X shape:', X.shape)\n",
    "print('y shape:', y.shape)\n",
    "\n",
    "mses = []\n",
    "mses_train = []\n",
    "for size in layer_sizes:\n",
    "    nn = MLPRegressor(hidden_layer_sizes=size, activation='relu', solver='adam', alpha=0.001)\n",
    "\n",
    "    avg_mse, avg_mse_train = analyze_nn(nn, X, y)\n",
    "    mses.append(avg_mse)\n",
    "    mses_train.append(avg_mse_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27939.91645767 25743.02748859 16171.83384873 17876.96682795\n",
      " 26031.7230173  26402.69629553]\n"
     ]
    }
   ],
   "source": [
    "print(np.sqrt(mses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (586, 5)\n",
      "y shape: (586,)\n",
      "Layer size (50, 50, 50) MSE:\n",
      "   val = 490386407.23 \n",
      " train = 246093184.54\n",
      "Layer size (100, 100, 100) MSE:\n",
      "   val = 262890774.44 \n",
      " train = 230559564.49\n",
      "Layer size (150, 150, 150) MSE:\n",
      "   val = 270196558.47 \n",
      " train = 219096771.69\n",
      "Layer size (200, 200, 200) MSE:\n",
      "   val = 278820576.41 \n",
      " train = 207854681.98\n"
     ]
    }
   ],
   "source": [
    "y = data_all[1:,0] # Number of tweets (except first)\n",
    "X = np.delete(data_all, -1, 0) # Delete last row\n",
    "\n",
    "X = scaler.fit_transform(X) # Transform data\n",
    "y = y\n",
    "\n",
    "layer_sizes = [(50,50,50), (100,100,100), (150,150,150), (200,200,200)]\n",
    "\n",
    "print('X shape:', X.shape)\n",
    "print('y shape:', y.shape)\n",
    "\n",
    "mses = []\n",
    "mses_train = []\n",
    "for size in layer_sizes:\n",
    "    nn = MLPRegressor(hidden_layer_sizes=size, activation='relu', solver='adam', alpha=0.001)\n",
    "\n",
    "    avg_mse, avg_mse_train = analyze_nn(nn, X, y)\n",
    "    mses.append(avg_mse)\n",
    "    mses_train.append(avg_mse_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22144.66995076 16213.90682217 16437.65672081 16697.92131998]\n"
     ]
    }
   ],
   "source": [
    "print(np.sqrt(mses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dmwang626/anaconda/lib/python3.5/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=MLPRegressor(activation='relu', alpha=0.001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'hidden_layer_sizes': [(50,), (100,), (150,), (200,), (250,), (300,), (50, 50), (100, 100), (150, 150), (200, 200), (250, 250), (300, 300), (50, 50, 50), (100, 100, 100), (150, 150, 150), (200, 200, 200), (250, 250, 250), (300, 300, 300), (50, 50, 50, 50), (100, 100, 100, 100), (150, 150...0, 150, 150, 150), (200, 200, 200, 200, 200), (250, 250, 250, 250, 250), (300, 300, 300, 300, 300)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='neg_mean_squared_error', verbose=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_before = data_aggregate_before[1:,0] # Number of tweets (except first)\n",
    "X_before = np.delete(data_aggregate_before, -1, 0) # Delete last row\n",
    "\n",
    "X = scaler.fit_transform(X_before) # Transform data\n",
    "y = y_before\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (150,), (200,), (250,), (300,),\n",
    "                           2*(50,), 2*(100,), 2*(150,), 2*(200,), 2*(250,), 2*(300,), \n",
    "                           3*(50,), 3*(100,), 3*(150,), 3*(200,), 3*(250,), 3*(300,),\n",
    "                           4*(50,), 4*(100,), 4*(150,), 4*(200,), 4*(250,), 4*(300,),\n",
    "                           5*(50,), 5*(100,), 5*(150,), 5*(200,), 5*(250,), 5*(300,)]\n",
    "}\n",
    "\n",
    "nn = MLPRegressor(activation='relu', solver='adam', alpha=0.001)\n",
    "clf_before = GridSearchCV(nn, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "clf_before.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layer_sizes': (200, 200)}\n",
      "-4680478.984374924\n"
     ]
    }
   ],
   "source": [
    "print(clf_before.best_params_)\n",
    "print(clf_before.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dmwang626/anaconda/lib/python3.5/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=MLPRegressor(activation='relu', alpha=0.001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'hidden_layer_sizes': [(50,), (100,), (150,), (200,), (250,), (300,), (50, 50), (100, 100), (150, 150), (200, 200), (250, 250), (300, 300), (50, 50, 50), (100, 100, 100), (150, 150, 150), (200, 200, 200), (250, 250, 250), (300, 300, 300), (50, 50, 50, 50), (100, 100, 100, 100), (150, 150...0, 150, 150, 150), (200, 200, 200, 200, 200), (250, 250, 250, 250, 250), (300, 300, 300, 300, 300)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='neg_mean_squared_error', verbose=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_during = data_aggregate_during[1:,0] # Number of tweets (except first)\n",
    "X_during = np.delete(data_aggregate_during, -1, 0) # Delete last row\n",
    "\n",
    "X = scaler.fit_transform(X_during) # Transform data\n",
    "y = y_during\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (150,), (200,), (250,), (300,),\n",
    "                           2*(50,), 2*(100,), 2*(150,), 2*(200,), 2*(250,), 2*(300,), \n",
    "                           3*(50,), 3*(100,), 3*(150,), 3*(200,), 3*(250,), 3*(300,),\n",
    "                           4*(50,), 4*(100,), 4*(150,), 4*(200,), 4*(250,), 4*(300,),\n",
    "                           5*(50,), 5*(100,), 5*(150,), 5*(200,), 5*(250,), 5*(300,)]\n",
    "}\n",
    "\n",
    "nn = MLPRegressor(activation='relu', solver='adam', alpha=0.001)\n",
    "clf_during = GridSearchCV(nn, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "clf_during.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layer_sizes': (250, 250, 250, 250, 250)}\n",
      "-37522544.31218094\n"
     ]
    }
   ],
   "source": [
    "print(clf_during.best_params_)\n",
    "print(clf_during.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dmwang626/anaconda/lib/python3.5/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=MLPRegressor(activation='relu', alpha=0.001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'hidden_layer_sizes': [(50,), (100,), (150,), (200,), (250,), (300,), (50, 50), (100, 100), (150, 150), (200, 200), (250, 250), (300, 300), (50, 50, 50), (100, 100, 100), (150, 150, 150), (200, 200, 200), (250, 250, 250), (300, 300, 300), (50, 50, 50, 50), (100, 100, 100, 100), (150, 150...0, 150, 150, 150), (200, 200, 200, 200, 200), (250, 250, 250, 250, 250), (300, 300, 300, 300, 300)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='neg_mean_squared_error', verbose=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_after = data_aggregate_after[1:,0] # Number of tweets (except first)\n",
    "X_after = np.delete(data_aggregate_after, -1, 0) # Delete last row\n",
    "\n",
    "X = scaler.fit_transform(X_after) # Transform data\n",
    "y = y_after\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (150,), (200,), (250,), (300,),\n",
    "                           2*(50,), 2*(100,), 2*(150,), 2*(200,), 2*(250,), 2*(300,), \n",
    "                           3*(50,), 3*(100,), 3*(150,), 3*(200,), 3*(250,), 3*(300,),\n",
    "                           4*(50,), 4*(100,), 4*(150,), 4*(200,), 4*(250,), 4*(300,),\n",
    "                           5*(50,), 5*(100,), 5*(150,), 5*(200,), 5*(250,), 5*(300,)]\n",
    "}\n",
    "\n",
    "nn = MLPRegressor(activation='relu', solver='adam', alpha=0.001)\n",
    "clf_after = GridSearchCV(nn, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "clf_after.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layer_sizes': (150, 150, 150, 150, 150)}\n",
      "-900373.8765929521\n"
     ]
    }
   ],
   "source": [
    "print(clf_after.best_params_)\n",
    "print(clf_after.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets/Testing/sample0_period1.txt\n",
      "Datasets/Testing/sample0_period2.txt\n",
      "Datasets/Testing/sample0_period3.txt\n",
      "Datasets/Testing/sample1_period1.txt\n",
      "Datasets/Testing/sample1_period2.txt\n",
      "Datasets/Testing/sample1_period3.txt\n",
      "Datasets/Testing/sample2_period1.txt\n",
      "Datasets/Testing/sample2_period2.txt\n",
      "Datasets/Testing/sample2_period3.txt\n"
     ]
    }
   ],
   "source": [
    "testing_dir = os.path.join(\"Datasets\", \"Testing\")\n",
    "pst_tz = pytz.timezone('America/Los_Angeles')\n",
    "\n",
    "# iterate over all hashtag files \n",
    "for root, dirs, files in os.walk(testing_dir, topdown=False):\n",
    "    for file in files:\n",
    "        print(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sample0_period1...\n",
      "Parsing sample0_period2...\n",
      "Parsing sample0_period3...\n",
      "Parsing sample1_period1...\n",
      "Parsing sample1_period2...\n",
      "Parsing sample1_period3...\n",
      "Parsing sample2_period1...\n",
      "Parsing sample2_period2...\n",
      "Parsing sample2_period3...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Parse files to get necessary data \"\"\"\n",
    "\n",
    "data_test_tweets = {}\n",
    "\n",
    "for root, dirs, files in os.walk(testing_dir, topdown=False):\n",
    "    for file in files:\n",
    "        filename = os.path.splitext(file)[0].replace('tweets_#', '')\n",
    "        print('Parsing {}...'.format(filename))\n",
    "        \n",
    "        data_test_tweets[filename] = []\n",
    "        \n",
    "        # open the file and read all lines:\n",
    "        with open(os.path.join(root, file), \"r\", encoding=\"utf-8\") as hashtag:\n",
    "            # read line-by-line\n",
    "            for line in hashtag:\n",
    "                json_obj = json.loads(line)\n",
    "                \n",
    "                # get desired statistics\n",
    "                citation_date = json_obj['citation_date'] # Unix time\n",
    "                num_retweets = json_obj['metrics']['citations']['total'] # Number of retweets for this tweet\n",
    "                num_followers = json_obj['author']['followers'] # Number of followers for tweeter\n",
    "                \n",
    "                data_test_tweets[filename].append([citation_date, num_retweets, num_followers])\n",
    "\n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Explicitly list out file names\n",
    "filenames = ['sample0_period1', 'sample0_period2', 'sample0_period3', \n",
    "             'sample1_period1', 'sample1_period2', 'sample1_period3', \n",
    "             'sample2_period1', 'sample2_period2', 'sample2_period3']\n",
    "\n",
    "# Make each value a numpy array\n",
    "for key in filenames:\n",
    "    data_test_tweets[key] = np.array(data_test_tweets[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample0_period1\n",
      "sample0_period2\n",
      "sample0_period3\n",
      "sample1_period1\n",
      "sample1_period2\n",
      "sample1_period3\n",
      "sample2_period1\n",
      "sample2_period2\n",
      "sample2_period3\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Organize the test set by file name into the correct format. Each value is n x 5, \n",
    "    where n is the number of time windows, and 5 is the number of features (same as above). \"\"\"\n",
    "\n",
    "# Initialize dictionary. Key is file name, value is a numpy array.\n",
    "data_test = {}\n",
    "for key in filenames: # Iterate through each test file\n",
    "    print(key)\n",
    "    temp = data_test_tweets[key] # Rename temp for simplicity\n",
    "    data_test[key] = np.zeros((6,5)) # Initialize array as zeros\n",
    "    \n",
    "    ftt = np.min(temp[:,0]) # Find (Unix) time of first tweet\n",
    "    start_time = ftt - (ftt % 3600) # Find the hour of the first tweet.\n",
    "    num_followers = {} # Initialize dictionary, to be used later to find max number of followers.\n",
    "    \n",
    "    for i in range(np.shape(temp)[0]): # Iterate through all tweets in specific file. \n",
    "        if int(key[-1]) == 2: # Check if the file is tweets from period 2.\n",
    "            item = int(((temp[i,0] - start_time) * 12) // 3600 - 6) # Find index number\n",
    "        else: # Period 1 or 3\n",
    "            item = int((temp[i,0] - start_time) // 3600)\n",
    "        data_test[key][item] += np.array([1, temp[i,1], temp[i,2], 0, 0]) # Update first 3 elements\n",
    "\n",
    "        dt_obj_pst = datetime.fromtimestamp(temp[i,0], pst_tz) \n",
    "        data_test[key][item][4] = int(datetime.strftime(dt_obj_pst, '%H')) # Find hour of tweet time\n",
    "        if item not in num_followers.keys():\n",
    "            num_followers[item] = []\n",
    "        num_followers[item].append(temp[i,2])\n",
    "    for i in num_followers.keys():\n",
    "        data_test[key][i][3] = np.max(num_followers[i]) # Update max number of followers.\n",
    "        \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample0_period1\n",
      "True: [ 52.  79.  94. 101. 122. 120.]\n",
      "Linear Regression: [486.95 462.29 504.15 532.9  519.66 446.83]\n",
      "K Neighbors: [200.   381.33 319.   961.17 415.33 940.33]\n",
      "LR MSE: 159301.9175395098\n",
      "KNR MSE: 277135.3935185185\n",
      " \n",
      "sample0_period2\n",
      "True: [3472. 3834. 2258. 1455. 1235. 1123.]\n",
      "Linear Regression: [5402.06 5620.64 4021.69 3120.12 2993.65 2834.92]\n",
      "K Neighbors: [ 8434.5  11071.67  3295.5   2096.5   2096.5   2096.5 ]\n",
      "LR MSE: 3137326.184178239\n",
      "KNR MSE: 13364673.004629627\n",
      " \n",
      "sample0_period3\n",
      "True: [59. 48. 94. 45. 77. 87.]\n",
      "Linear Regression: [514.26 344.25 355.09 302.99 314.56 290.17]\n",
      "K Neighbors: [671.    35.83  35.83  35.83 173.17  35.83]\n",
      "LR MSE: 87910.63718106104\n",
      "KNR MSE: 65004.24537037036\n",
      " \n",
      "sample1_period1\n",
      "True: [203. 180. 202. 294. 555. 846.]\n",
      "Linear Regression: [ 591.83  567.96  576.28  657.88  824.76 1038.6 ]\n",
      "K Neighbors: [ 216.33  289.33  519.33 1011.83  724.67  972.17]\n",
      "LR MSE: 114009.27578329499\n",
      "KNR MSE: 112136.91666666667\n",
      " \n",
      "sample1_period2\n",
      "True: [960. 995. 870. 960. 861. 903.]\n",
      "Linear Regression: [1734.13 1817.2  1674.23 1979.44 1640.55 1732.81]\n",
      "K Neighbors: [2096.5  2096.5  2096.5  1033.17 2096.5  2096.5 ]\n",
      "LR MSE: 709604.8297774276\n",
      "KNR MSE: 1160915.4351851852\n",
      " \n",
      "sample1_period3\n",
      "True: [58. 87. 43. 27. 44. 46.]\n",
      "Linear Regression: [ 338.21   37.53  -79.15 -108.7   363.08  343.25]\n",
      "K Neighbors: [682.17 529.67  35.83  35.83  35.83  35.83]\n",
      "LR MSE: 50745.506821512965\n",
      "KNR MSE: 97639.54166666667\n",
      " \n",
      "sample2_period1\n",
      "True: [401. 141. 102. 144. 104.  61.]\n",
      "Linear Regression: [605.77 478.23 456.98 477.07 432.08 419.06]\n",
      "K Neighbors: [381.33 287.17 101.17 101.17 233.33  86.17]\n",
      "LR MSE: 104741.23993364153\n",
      "KNR MSE: 6824.5555555555575\n",
      " \n",
      "sample2_period2\n",
      "True: [24. 19. 25. 27. 29. 28.]\n",
      "Linear Regression: [317.29 312.71 323.97 339.72 327.36 370.48]\n",
      "K Neighbors: [2096.5 2096.5 2096.5 2096.5 2096.5 2080.5]\n",
      "LR MSE: 94295.89123629082\n",
      "KNR MSE: 4278752.916666667\n",
      " \n",
      "sample2_period3\n",
      "True: [81. 90. 40. 58. 87. 43.]\n",
      "Linear Regression: [ 48.52  56.63 -16.71 338.21  37.53 -79.15]\n",
      "K Neighbors: [ 35.83 408.83  35.83 682.17 529.67  35.83]\n",
      "LR MSE: 16878.505847547716\n",
      "KNR MSE: 114883.54166666664\n",
      " \n"
     ]
    }
   ],
   "source": [
    "\"\"\" Perform regression on the test set to predict the number \n",
    "    of tweets in the next time window (either 1 hour or 5 minutes) \"\"\"\n",
    "\n",
    "for key in filenames: # Iterate through all files\n",
    "    print(key)\n",
    "    lr = LinearRegression() # Instantiate a linear regressor\n",
    "    knr = KNeighborsRegressor(n_neighbors=6) # Instantiate a k nearest neighbors regressor\n",
    "    \n",
    "    # Create train set and train labels\n",
    "    if key[-1] == '1': # Check if file is from period 1\n",
    "        y = data_aggregate_before[1:, 0] # Get # of tweets for next time window\n",
    "        X = np.delete(data_aggregate_before, -1, 0) # Get all training points except last row\n",
    "    elif key[-1] == '2': # Check f file is from period 2\n",
    "        y = data_aggregate_during[1:, 0]\n",
    "        X = np.delete(data_aggregate_during, -1, 0)\n",
    "    else: # Else file is from period 3\n",
    "        y = data_aggregate_after[1:, 0]\n",
    "        X = np.delete(data_aggregate_after, -1, 0)\n",
    "    \n",
    "    lr.fit(X, y)\n",
    "    knr.fit(X, y)\n",
    "    predicted_lr = lr.predict(data_test[key])\n",
    "    predicted_knr = knr.predict(data_test[key])\n",
    "    print('True:', data_test[key][:,0])\n",
    "    print('Linear Regression:', np.around(predicted_lr, 2))\n",
    "    print('K Neighbors:', np.around(predicted_knr, 2))\n",
    "    print('LR MSE:', mean_squared_error(predicted_lr, data_test[key][:,0]))\n",
    "    print('KNR MSE:', mean_squared_error(predicted_knr, data_test[key][:,0]))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Organize training data to combine 6 time windows. There will be 30 features (6 x original 5). \"\"\"\n",
    "\n",
    "# Time period 1 (before Feb 1, 8am)\n",
    "data_aggregate_before_6x = np.zeros((data_aggregate_before.shape[0] // 6, 30))\n",
    "y_before_6x = np.zeros(data_aggregate_before.shape[0] // 6)\n",
    "try: # Catch IndexError, if number of windows is not divisible by 6\n",
    "    for i in range(data_aggregate_before.shape[0]): # Iterate through all rows in original aggregated set\n",
    "        data_aggregate_before_6x[i//6, 5*(i%6):5*(i%6)+5] = data_aggregate_before[i] # Update 5 elements \n",
    "        y_before_6x[i//6] += data_aggregate_before[i,0] # Get corresponding labels (# of tweets)\n",
    "except IndexError:\n",
    "    pass\n",
    "\n",
    "# Time period 2\n",
    "data_aggregate_during_6x = np.zeros((data_aggregate_during.shape[0] // 6, 30))\n",
    "y_during_6x = np.zeros(data_aggregate_during.shape[0] // 6)\n",
    "try:\n",
    "    for i in range(data_aggregate_during.shape[0]):\n",
    "        data_aggregate_during_6x[i//6, 5*(i%6):5*(i%6)+5] = data_aggregate_during[i]\n",
    "        y_during_6x[i//6] += data_aggregate_during[i,0]\n",
    "except IndexError:\n",
    "    pass\n",
    "\n",
    "# Time period 3\n",
    "data_aggregate_after_6x = np.zeros((data_aggregate_after.shape[0] // 6, 30))\n",
    "y_after_6x = np.zeros(data_aggregate_after.shape[0] // 6)\n",
    "try:\n",
    "    for i in range(data_aggregate_after.shape[0]):\n",
    "        data_aggregate_after_6x[i//6, 5*(i%6):5*(i%6)+5] = data_aggregate_after[i]\n",
    "        y_after_6x[i//6] += data_aggregate_after[i,0]\n",
    "except IndexError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reshape test set into corresponding shape\n",
    "data_test_6x = {}\n",
    "for key in filenames:\n",
    "    data_test_6x[key] = np.reshape(data_test[key], (1,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample0_period1\n",
      "True: 568.0\n",
      "Linear Regressor: [568.]\n",
      "K Neighbors: [2800.5]\n",
      " \n",
      "sample0_period2\n",
      "True: 13377.0\n",
      "Linear Regressor: [13407.92]\n",
      "K Neighbors: [21915.67]\n",
      " \n",
      "sample0_period3\n",
      "True: 410.0\n",
      "Linear Regressor: [436.2]\n",
      "K Neighbors: [3258.]\n",
      " \n",
      "sample1_period1\n",
      "True: 2280.0\n",
      "Linear Regressor: [2280.]\n",
      "K Neighbors: [3549.83]\n",
      " \n",
      "sample1_period2\n",
      "True: 5549.0\n",
      "Linear Regressor: [5570.54]\n",
      "K Neighbors: [20490.5]\n",
      " \n",
      "sample1_period3\n",
      "True: 305.0\n",
      "Linear Regressor: [432.34]\n",
      "K Neighbors: [3073.67]\n",
      " \n",
      "sample2_period1\n",
      "True: 953.0\n",
      "Linear Regressor: [953.]\n",
      "K Neighbors: [1810.33]\n",
      " \n",
      "sample2_period2\n",
      "True: 152.0\n",
      "Linear Regressor: [187.62]\n",
      "K Neighbors: [10814.17]\n",
      " \n",
      "sample2_period3\n",
      "True: 399.0\n",
      "Linear Regressor: [473.28]\n",
      "K Neighbors: [3409.]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "\"\"\" Perform regression on the test set to predict the total number of tweets in the 6x window. \"\"\"\n",
    "\n",
    "for key in filenames:\n",
    "    print(key)\n",
    "    lr = LinearRegression()\n",
    "    knr = KNeighborsRegressor(n_neighbors=6)\n",
    "        \n",
    "    if key[-1] == '1':\n",
    "        lr.fit(data_aggregate_before_6x, y_before_6x)\n",
    "        knr.fit(data_aggregate_before_6x, y_before_6x)\n",
    "    elif key[-1] == '2':\n",
    "        lr.fit(data_aggregate_during_6x, y_during_6x)\n",
    "        knr.fit(data_aggregate_during_6x, y_during_6x)\n",
    "    else:\n",
    "        lr.fit(data_aggregate_after_6x, y_after_6x)\n",
    "        knr.fit(data_aggregate_after_6x, y_after_6x)\n",
    "    predicted_lr = lr.predict(data_test_6x[key])\n",
    "    predicted_knr = knr.predict(data_test_6x[key])\n",
    "\n",
    "    print('True:', np.sum(data_test[key][:,0]))\n",
    "    print('Linear Regressor:', np.around(predicted_lr, 2))\n",
    "    print('K Neighbors:', np.around(predicted_knr, 2))\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
